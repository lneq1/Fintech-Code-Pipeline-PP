# Fintech-Code-Pipeline-PP
The two files, eda.ipynb (exploratory data analysis) and main.ipynb, document how we analyzed a sleep-quality dataset and compared Multiple Linear Regression with a Random Forest model.

In this project, our goal was to compare Multiple Linear Regression and a Random Forest Regressor on a specific sleep-quality dataset, and to understand how both models behave under the same conditions. To do this systematically, we first worked in eda.ipynb, where we explored and prepared the data in detail. The dataset contains information from 100 individuals and includes a mixture of lifestyle and health-related variables. During the exploratory analysis, we examined variable distributions, correlations, and potential sources of redundancy. A key discovery was the extremely strong correlation between Steps and Calories, which created heavy multicollinearity. Because this directly affects linear models, we applied VIF (Variance Inflation Factor) analysis to identify and remove problematic features. After iterative feature selection, we arrived at four stable predictors: Dietary Habits, Age, Gender, and Sleep Disorders. With multicollinearity resolved, the dataset was ready for modelling.

The modelling process took place in main.ipynb. We first trained a Multiple Linear Regression model on the cleaned feature set. Thanks to the linear nature of the data, the model performed extremely well, achieving high accuracy on both training and test data. The coefficients showed clear relationships: for example, the presence of sleep disorders and unhealthy dietary habits strongly reduced the predicted sleep-quality score. Because linear regression requires low multicollinearity and works best with linearly structured data, this model benefited directly from the earlier VIF-based feature selection.

We then trained a Random Forest Regressor, optimized through cross-validation. Unlike linear regression, the Random Forest does not suffer from multicollinearity and can even benefit from correlated variables by capturing non-linear relationships and feature interactions. When we included all features (even the correlated ones), the Random Forest slightly outperformed the linear model. However, when restricted to the same four selected features, its performance dropped. This demonstrated an important limitation: while feature reduction is essential for linear models, it can remove useful interaction patterns that Random Forests rely on.

Overall, the project illustrates how both models respond to the structure of this specific dataset. Linear Regression excels when the data is clean, low in multicollinearity, and primarily linear. Random Forest, on the other hand, is more flexible and robust to correlated features, and it captures non-linear patterns that linear models cannot. By comparing both models under identical conditions, we highlight their strengths, their limitations, and the role of feature selection in shaping their performance. 
